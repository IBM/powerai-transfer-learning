{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in TensorFlow with Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Transfer learning is the process of taking a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and “fine-tuning” the model with your own dataset. The idea is that this pre-trained model will act as a feature extractor. You will remove the last layer of the network and replace it with your own classifier (depending on what your problem space is). You then freeze the weights of all the other layers and train the network normally (Freezing the layers means not changing the weights during gradient descent/optimization).\n",
    "\n",
    "For this experiment we used Google's Inception-V3 pretrained model for Image Classification. This model consists of two parts:\n",
    "    - Feature extraction part with a convolutional neural network.\n",
    "    - Classification part with fully-connected and softmax layers.\n",
    "The pre-trained Inception-v3 model achieves state-of-the-art accuracy for recognizing general objects with 1000 classes. The model extracts general features from input images in the first part and classifies them based on those features in the second part.\n",
    "\n",
    "We will use this pre-trained model and re-train it it to classify houses with or without swimming pools. \n",
    "\n",
    "The following chart shows how the data flows in the Inception v3 model, which is a Convolutional Neural Network with many layers and a complicated structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/inception_flowchart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In transfer learning, when you build a new model to classify your original dataset, you reuse the feature extraction part and re-train the classification part with your dataset. Since you don't have to train the feature extraction part (which is the most complex part of the model), you can train the model with less computational resources and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/inception_transfer_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this experiment, we built two small image datasets (less than 600 images) -- one with images of houses without swimming pools and another one with images of houses with swimming pools.\n",
    "\n",
    "After downloading the images, we took an extra step to visualize the images and remove the false positives. All the images were then saved in two different directories identifying the proper classification.\n",
    "\n",
    "In the public GitHub repo we only provided a subset of the images, but we also provided bottleneck files to represent the rest of the images in the dataset. It might be worth noting that most of the bottleneck files represent aerial view images. It would not be surprising if we recognize pools better from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing\n",
    "\n",
    "The raw images need to be resized to 299 x 299. The notebook code will resize the raw images into a working directory. We're also able to reuse resized images like the one below which is stored in a folder in the repo. If you have your own large dataset, you might want to do the resize once and store the resized images to use instead of the raw images.\n",
    "\n",
    "#### For example, running the notebook resizes this image:\n",
    "\n",
    "<img src=\"../data/images/house_with_pool/house-429353_960_720.jpg\">\n",
    "\n",
    "#### To this 299 x 299 image: \n",
    "\n",
    "<img src=\"../data/images_resized/house_with_pool/house-429353_960_720.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Python packages\n",
    "The PowerAI TensorFlow already has TensorFlow and PIL, but we need python-resize-image for the image resizing step.\n",
    "Run this cell at least once. You might need to restart your kernel after the install. Use the Kernel menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user python-resize-image==1.1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We put all the imports at the top of the code, because this is what most Python developers would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import image retraining function definitions\n",
    "\n",
    "The image_retraining example module from TensorFlow can be used from a notebook by importing it and calling the functions directly. A FLAGS object is used in the module. We just create one and set it in the `Parameters` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from image_retraining import retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Many of the parameters can be changed if you choose to experiment with different images and training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set DEBUG to True for more output\n",
    "DEBUG = False\n",
    "\n",
    "# Expect image files to always end with one of these\n",
    "JPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\n",
    "\n",
    "# Raw input images come from this dir in the git repo (or you can customize this to point to a new dir).\n",
    "# Only JPEG images are used. We will resize these images before using them.\n",
    "image_dir = '../data/images'\n",
    "\n",
    "# We kept some images separate for our manual testing at the end.\n",
    "test_images_dir = '../data/test_images'\n",
    "\n",
    "# If stored_images_resized, images here have already been resized are can be used w/o re-resizing\n",
    "stored_images_resized = '../data/images_resized'  # set to None to ignore\n",
    "\n",
    "# If stored_bottlenecks, supplement the image_dir collection with persisted bottlenecks from this dir\n",
    "stored_bottlenecks = '../data/bottlenecks'  # set to None to ignore\n",
    "\n",
    "# Working files are in /tmp by default\n",
    "tmp_dir = '/tmp'\n",
    "bottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\n",
    "images_resized_dir = os.path.join(tmp_dir, 'images_resized')\n",
    "summaries_dir = os.path.join(tmp_dir, 'retrain_logs')\n",
    "\n",
    "# Download the original inception model to/from here\n",
    "model_dir = os.path.join(tmp_dir, 'inception')\n",
    "inception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "\n",
    "# Store the graph before and after training\n",
    "output_graph_orig = \"output_graph_orig.pb\"\n",
    "output_graph = \"output_graph.pb\"\n",
    "output_labels = \"output_labels.txt\"\n",
    "\n",
    "# Training params\n",
    "architecture = 'inception_v3'\n",
    "final_tensor_name = \"final_result\"\n",
    "how_many_training_steps = 500\n",
    "learning_rate = 0.01\n",
    "testing_percentage = 10\n",
    "validation_percentage = 10\n",
    "eval_step_interval = 10\n",
    "train_batch_size = 100\n",
    "test_batch_size = -1\n",
    "validation_batch_size = 100\n",
    "print_misclassified_test_images = False\n",
    "\n",
    "# Since we are using persisted bottleneck files, we won't play with distortion.\n",
    "# Distortion would have limited impact with our small set of image files.\n",
    "flip_left_right = False\n",
    "random_crop = 0\n",
    "random_scale = 0\n",
    "random_brightness = 0\n",
    "\n",
    "# Download once and re-use by default\n",
    "force_inception_download = False\n",
    "\n",
    "# Create a FLAGS object with these attributes\n",
    "FLAGS = type('FlagsObject', (object,), {\n",
    "    'architecture': architecture,\n",
    "    'model_dir': model_dir,\n",
    "    'intermediate_store_frequency': 0,\n",
    "    'summaries_dir': summaries_dir,\n",
    "    'learning_rate': learning_rate,\n",
    "    'image_dir': images_resized_dir,\n",
    "    'testing_percentage': testing_percentage,\n",
    "    'validation_percentage': validation_percentage,\n",
    "    'random_scale': random_scale,\n",
    "    'random_crop': random_crop,\n",
    "    'flip_left_right': flip_left_right,\n",
    "    'random_brightness': random_brightness,\n",
    "    'bottleneck_dir': bottleneck_dir,\n",
    "    'final_tensor_name': final_tensor_name,\n",
    "    'how_many_training_steps': how_many_training_steps,\n",
    "    'train_batch_size': train_batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'eval_step_interval': eval_step_interval,\n",
    "    'validation_batch_size': validation_batch_size,\n",
    "    'print_misclassified_test_images': print_misclassified_test_images,\n",
    "    'output_graph': output_graph,\n",
    "    'output_labels': output_labels\n",
    "})\n",
    "\n",
    "# Setting the FLAGS in retrain allows us to call the functions directly\n",
    "retrain.FLAGS = FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the Inception model once and reuse it (set the flag and clobber it each time).\n",
    "if force_inception_download and os.path.isdir(model_dir):    \n",
    "    shutil.rmtree(model_dir)\n",
    "retrain.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the images\n",
    "\n",
    "The Inception model requires 299 X 299 pixel sizes.\n",
    "First copy the files from `stored_images_resized` into `images_resized_dir`.\n",
    "With these stored images that are already resized, we don't need to repeat the process.\n",
    "Next copy and resize the remaining raw images from `image_dir` into `images_resized_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resize_images(src_dir, dest_dir):\n",
    "    if not os.path.isdir(src_dir):\n",
    "        raise Exception(src_dir + \" is not a directory\")\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    raw_images = {image for image in os.listdir(src_dir) if image.endswith(\n",
    "        JPEG_EXTENSIONS)}\n",
    "    dest_images = {image for image in os.listdir(dest_dir)}\n",
    "\n",
    "    # Resize the ones that are not already in the dest dir\n",
    "    for image in raw_images - dest_images:\n",
    "        if DEBUG:\n",
    "            print(\"Resizing \" + image)\n",
    "        resize_image(image, src_dir, dest_dir)\n",
    "\n",
    "\n",
    "def resize_image(image_file, src_dir, dest_dir):\n",
    "    in_file = os.path.join(src_dir, image_file)\n",
    "    with open(in_file, 'r+b') as fd_img:\n",
    "        with Image.open(fd_img) as img:\n",
    "            resized_image = resizeimage.resize_contain(\n",
    "                img, [299, 299]).convert(\"RGB\")\n",
    "            resized_image.save(os.path.join(dest_dir, image_file), img.format)\n",
    "\n",
    "# Use a fresh working dir for the resized images\n",
    "if os.path.isdir(images_resized_dir):\n",
    "    shutil.rmtree(images_resized_dir)\n",
    "os.mkdir(images_resized_dir)\n",
    "    \n",
    "subdirs = ('house_with_pool', 'house_without_pool')\n",
    "\n",
    "# Copy in the image files\n",
    "for subdir in subdirs:\n",
    "    dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "      \n",
    "    # Copy the already resized files first, if any, from the repo or a custom dir\n",
    "    if stored_images_resized:\n",
    "        source_dir = os.path.join(stored_images_resized, subdir)\n",
    "        if os.path.isdir(source_dir):\n",
    "            for f in os.listdir(source_dir):\n",
    "                path = os.path.join(source_dir, f)\n",
    "                if (os.path.isfile(path)):\n",
    "                    shutil.copy(path, dest_dir)\n",
    "                    \n",
    "    # Copy/resize the remaining raw images into the images_resized_dir(s)\n",
    "    resize_images(os.path.join(image_dir, subdir), dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset images\n",
    "\n",
    "Since Jupyter notebooks are great at showing markdown documentation as well as code and output, we can look at some of the images here.\n",
    "\n",
    "To visualize a different image, double click on the displayed image below, the markdown text will show up. Change the image file name to display another one.\n",
    "\n",
    "Some false positives have been removed from our dataset, but it is still interesting to see which images are harder to classify. Lakes and ponds would be something to look into. Some of the lower confidence numbers seem to come from shapes that resemble pools (and not bodies of water).\n",
    "\n",
    "Removing false positives can often help the training but if we want to improve the training to classify those images with confidence as well, then we might just need a bigger dataset with a good amount of relevant examples to learn from.\n",
    "\n",
    "<img src=\"../data/images/house_without_pool/giethoorn-2368494__340.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy stored bottleneck files\n",
    "Many previously calculated bottleneck files are stored in `stored_bottlenecks`\n",
    "to improve our dataset size and reduce processing time. Here we copy them into the working `bottleneck_dir`.\n",
    "We also create a placeholder image file so that they are included in our image lists for training, validation,\n",
    "and testing. The placeholder contents won't be used because the bottleneck is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a fresh working dir for the bottleneck files  \n",
    "if os.path.isdir(bottleneck_dir):    \n",
    "    shutil.rmtree(bottleneck_dir)\n",
    "os.mkdir(bottleneck_dir)\n",
    "\n",
    "subdirs = ('house_with_pool', 'house_without_pool')\n",
    "\n",
    "# Copy in the stored bottleneck files\n",
    "for subdir in subdirs:\n",
    "    dest_dir = os.path.join(bottleneck_dir, subdir)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    image_dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "\n",
    "    if stored_bottlenecks:\n",
    "        source_dir = os.path.join(stored_bottlenecks, subdir)\n",
    "        if os.path.isdir(source_dir):\n",
    "            for f in os.listdir(source_dir):\n",
    "                path = os.path.join(source_dir, f)\n",
    "                if (os.path.isfile(path)):\n",
    "                    # Copy the persisted bottleneck to bottlenecks dir\n",
    "                    shutil.copy(path, dest_dir)\n",
    "                    # \"touch\" the file (w/o the .txt) to create a placeholder image\n",
    "                    # This image file will only be used to build the lists.\n",
    "                    if DEBUG:\n",
    "                        print(\"Creating placeholder image at %s\" % os.path.join(image_dest_dir, f[:-4]))\n",
    "                    open(os.path.join(image_dest_dir, f[:-4]), 'a').close\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining\n",
    "\n",
    "The following code demonstrates how to take an Inception v3 architecture model trained on\n",
    "ImageNet images, and train a new top layer that can recognize other classes of\n",
    "images.\n",
    "\n",
    "The top layer receives as input a 2048-dimensional vector for each image. We\n",
    "train a softmax layer on top of this representation. Assuming the softmax layer\n",
    "contains N labels, this corresponds to learning N + 2048*N model parameters\n",
    "corresponding to the learned biases and weights.\n",
    "\n",
    "We have a folder with two subfolders called **house_with_pool** and **house_without_pool**.\n",
    "JPEG images have been selected for training and placed in the proper folder.\n",
    "The subfolder names are important, since they define what label is applied to each image, but the filenames themselves don't matter. The label for each image is taken from the name of the subfolder it's in. This produces a new model file that can be loaded and run by any TensorFlow program.\n",
    "\n",
    "In addition to the small sample of images, we have a larger set of bottlenecks. These were captured from\n",
    "images used in earlier runs and will be used here to increase the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "  # Setup the directory we'll write summaries to for TensorBoard\n",
    "  if tf.gfile.Exists(FLAGS.summaries_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n",
    "\n",
    "  # Set up the pre-trained graph.\n",
    "  graph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n",
    "      retrain.create_inception_graph())\n",
    "\n",
    "  # Look at the folder structure, and create lists of all the images.\n",
    "  # This is why we use placeholder images when we reuse bottleneck files.\n",
    "  image_lists = retrain.create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n",
    "                                   FLAGS.validation_percentage)\n",
    "  class_count = len(image_lists.keys())\n",
    "  if class_count == 0:\n",
    "    raise Exception('No valid folders of images found at ' + FLAGS.image_dir)\n",
    "  if class_count == 1:\n",
    "    raise Exception('Only one valid folder of images found at ' + FLAGS.image_dir +\n",
    "          ' - multiple classes are needed for classification.')\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    # Calculate and cache bottleneck files based on the resized images\n",
    "    retrain.cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\n",
    "                    FLAGS.bottleneck_dir, jpeg_data_tensor,\n",
    "                    bottleneck_tensor)\n",
    "\n",
    "    # Add the new layer that we'll be training.\n",
    "    (train_step, cross_entropy, bottleneck_input, ground_truth_input,\n",
    "     final_tensor) = retrain.add_final_training_ops(len(image_lists.keys()),\n",
    "                                            FLAGS.final_tensor_name,\n",
    "                                            bottleneck_tensor)\n",
    "\n",
    "    # Create the operations we need to evaluate the accuracy of our new layer.\n",
    "    evaluation_step, prediction = retrain.add_evaluation_step(\n",
    "        final_tensor, ground_truth_input)\n",
    "\n",
    "    # Merge all the summaries and write them out to the summaries_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                         sess.graph)\n",
    "\n",
    "    validation_writer = tf.summary.FileWriter(\n",
    "        FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "    # Set up all our weights to their initial default values.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Save the original graph, so we can compare results later!\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph.as_graph_def(), [final_tensor_name])\n",
    "    with gfile.FastGFile(output_graph_orig, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    # Run the training!\n",
    "    for i in range(FLAGS.how_many_training_steps):\n",
    "\n",
    "      (train_bottlenecks, train_ground_truth, _) = retrain.get_random_cached_bottlenecks(\n",
    "             sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "             bottleneck_tensor)\n",
    "    \n",
    "      # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "      # step. Capture training summaries for TensorBoard with the `merged` op.\n",
    "      train_summary, _ = sess.run(\n",
    "          [merged, train_step],\n",
    "          feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                     ground_truth_input: train_ground_truth})\n",
    "      train_writer.add_summary(train_summary, i)\n",
    "\n",
    "      # Every so often, print out how well the graph is training.\n",
    "      is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
    "      if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "        train_accuracy, cross_entropy_value = sess.run(\n",
    "            [evaluation_step, cross_entropy],\n",
    "            feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                       ground_truth_input: train_ground_truth})\n",
    "        print('%s: Step %d: Train accuracy = %.1f%%' % (datetime.now(), i,\n",
    "                                                        train_accuracy * 100))\n",
    "        print('%s: Step %d: Cross entropy = %f' % (datetime.now(), i,\n",
    "                                                   cross_entropy_value))\n",
    "        validation_bottlenecks, validation_ground_truth, _ = (\n",
    "            retrain.get_random_cached_bottlenecks(\n",
    "                sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
    "                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                bottleneck_tensor))\n",
    "        # Run a validation step and capture training summaries for TensorBoard\n",
    "        # with the `merged` op.\n",
    "        validation_summary, validation_accuracy = sess.run(\n",
    "            [merged, evaluation_step],\n",
    "            feed_dict={bottleneck_input: validation_bottlenecks,\n",
    "                       ground_truth_input: validation_ground_truth})\n",
    "        validation_writer.add_summary(validation_summary, i)\n",
    "        print('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "              (datetime.now(), i, validation_accuracy * 100,\n",
    "               len(validation_bottlenecks)))\n",
    "\n",
    "    # We've completed all our training, so run a final test evaluation on\n",
    "    # some new images we haven't used before.\n",
    "    test_bottlenecks, test_ground_truth, test_filenames = (\n",
    "        retrain.get_random_cached_bottlenecks(sess, image_lists, FLAGS.test_batch_size,\n",
    "                                      'testing', FLAGS.bottleneck_dir,\n",
    "                                      FLAGS.image_dir, jpeg_data_tensor,\n",
    "                                      bottleneck_tensor))\n",
    "    test_accuracy, predictions = sess.run(\n",
    "        [evaluation_step, prediction],\n",
    "        feed_dict={bottleneck_input: test_bottlenecks,\n",
    "                   ground_truth_input: test_ground_truth})\n",
    "    print('Final test accuracy = %.1f%% (N=%d)' % (\n",
    "        test_accuracy * 100, len(test_bottlenecks)))\n",
    "\n",
    "    if FLAGS.print_misclassified_test_images:\n",
    "      print('=== MISCLASSIFIED TEST IMAGES ===')\n",
    "      for i, test_filename in enumerate(test_filenames):\n",
    "        if predictions[i] != test_ground_truth[i].argmax():\n",
    "          print('%70s  %s' % (test_filename,\n",
    "                              list(image_lists.keys())[predictions[i]]))\n",
    "\n",
    "    # Write out the trained graph and labels with the weights stored as\n",
    "    # constants.\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n",
    "    with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:\n",
    "      f.write(output_graph_def.SerializeToString())\n",
    "    with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
    "      f.write('\\n'.join(image_lists.keys()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The final test accuracy is **~85%** for our two classes **house_with_pool** and **house_without_pool** which is quite substantial given our training set contained less than 600 images. This is where Transfer Learning really shines. We used the trained Inception Model which already had learned to recognize lines, shapes and other features that increase in abstraction as we move toward the final layers of the model. We only had to retrain the last layers where we supplied training images of houses with and without pools.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to give it a try?\n",
    "\n",
    "We added some test images that you can use to test the model or you can download your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test images with pools:\n",
    "<img src=\"../data/test_images/house_with_pool/home-2008825__340.jpg\">\n",
    "<img src=\"../data/test_images/house_with_pool/villa-2366288__340.jpg\">\n",
    "### Test images without pools:\n",
    "<img src=\"../data/test_images/house_without_pool/holiday-house-177401__340.jpg\">\n",
    "<img src=\"../data/test_images/house_without_pool/weathered-2139859__340.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference engine with the original graph file and then with the retrained graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test with the test_images subdirs\n",
    "for graph in (output_graph_orig, output_graph):\n",
    "    print(\"\\nTesting with graph=%s\\n\" % graph)\n",
    "    for subdir in ('house_with_pool', 'house_without_pool'):\n",
    "        test_dir = os.path.join(test_images_dir, subdir)\n",
    "        for f in os.listdir(test_dir):\n",
    "            if f.endswith(JPEG_EXTENSIONS):\n",
    "                tf.reset_default_graph()\n",
    "                image = os.path.join(test_dir, f)\n",
    "                print(image)\n",
    "                %run ../image_retraining/label_image.py --image=$image --graph=$graph --labels=$output_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The original results are not much better than a coin flip. This is the expected result as the Inception V3 model has not been trained for houses with or without pools.\n",
    "\n",
    "The new graph classifies the images correctly and with significant confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I hope that you are now able to apply pre-trained models to your problem statements. Be sure that the pre-trained model you have selected has been trained on a similar dataset as the one that you wish to use it on. There are various architectures people have tried on different types of datasets and I strongly encourage you to go through these architectures and apply them to your own problem statements.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
